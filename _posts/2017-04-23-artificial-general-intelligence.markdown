---
layout: post
title:  "Artificial General Intelligence"
date:   2017-04-23 00:33:14 +0530
categories: AGI, Artificial General Intelligence, GoodAi, 
---

#### Task
- Try listing down the various modules involved in a brain - Deepmind video had a slide
- Try fitting module for each and start developing code solving the problem at hand
- 

#### Genreral AI Challenge
- https://drive.google.com/file/d/0B820uHFOHp0mODQ3SFpzV01JTnc/view - Evaluation metric
- https://www.general-ai-challenge.org/active-rounds

#### Gradual Learning
- Incremental Learning, Curriculam learning, Transfer learning, cummulative learning
- REcursive self-improvement - builiding a architecture to learn the next architecture
- 

#### List of important links:
- https://www.goodai.com/school-for-ai
- Differential Neural computer:
	- https://github.com/dsindex/blog/wiki/%5Bdnc%5D-Differentiable-Neural-Computer
	- http://www.readcube.com/articles/10.1038/nature20101?shared_access_token=3UerOr1f0fy3oL_CytWdxtRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSujsARxGW1q2qxK0cTqi1Bup-nSH200cGUW_ET9MIG_6rvvXTcoxOnAX6B4E8dQs4FQ-yScxXe8EB0XnzqbUw3Qw%3D%3D - Nature Paper
	- https://www.youtube.com/watch?v=r5XKzjTFCZQ - Siraj You Tube video
	- https://www.youtube.com/watch?v=xILOeonmQi4 - Artificial Intelligence Pioneer Yoshua Bengio: Creating Human Level AI
	- Gated Graph Sequence Neural Networks - https://arxiv.org/abs/1511.05493
	- https://news.ycombinator.com/item?id=12694779
	- Deep Mind Blog: https://deepmind.com/blog/differentiable-neural-computers/
- https://arxiv.org/abs/1410.4615 - Learning to Execute
- Curriculum Learning - https://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf
- General Game Playing competition AI:  http://www.gvgai.net/
- The 2014 General Video Game Playing Competition - http://ieeexplore.ieee.org/document/7038214/

- https://github.com/facebookresearch/CommAI-env

- Youtube Discussions:
	- https://www.youtube.com/watch?v=wWklO5ACvUQ - Recent stream. Worth the watch
	- https://www.youtube.com/watch?v=Wy9ZoPpgtjU - To be looked into

- KPI - Key Performance Indicators
	- GAP Analysis: organizational managers can use to work out the size, and sometimes the shape, of the strategic tasks to be undertaken in order to move from its current state to a desired, future state.

#### Probably important but not read
- http://openml2017dev.openml.org/
- https://chatbotslife.com/why-meta-learning-is-crucial-for-further-advances-of-artificial-intelligence-c2df55959adf

#### Understanding the framework
- https://arxiv.org/pdf/1611.00685.pdf
- Possible future references:
	- [Building Machines That Learn and Think Like People](https://arxiv.org/pdf/1604.00289.pdf)
	- [A Roadmap towards Machine Intelligence](https://arxiv.org/pdf/1511.08130.pdf)
	- [Russell's Logical Atomism](https://plato.stanford.edu/archives/spr2016/entries/logical-atomism/)
#### Reference
- https://inclass.kaggle.com/c/forecasting-complex-seasonal-timeseries

#### InfoGAN:
- [OpenAI GAN](https://blog.openai.com/generative-models/)
	- [GAN](https://arxiv.org/pdf/1406.2661.pdf)
- [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/pdf/1606.03657.pdf)
- [TensorFlow Implimentation Git](https://github.com/openai/InfoGAN)
- [Keras Implementation Git](https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/InfoGAN)

#### Other important links
- [Various implementation of papers](https://github.com/tdeboissiere/DeepLearningImplementations)
- [fchollet twitter](https://twitter.com/fchollet?lang=en)
- [Reference For various deep learning](http://p.migdal.pl/2017/04/30/teaching-deep-learning.html)
#### Reference
- General AI challenge:
	- [GoodAi: General AI Challenge - Round 1 (gh)](https://github.com/general-ai-challenge/Round1)
		- [Specification](https://mirror.general-ai-challenge.org/challenge_first_round_specifications.pdf)
		- [](https://drive.google.com/file/d/0B820uHFOHp0mODQ3SFpzV01JTnc/view)
	- [Facebook: CommAI-env (gh)](https://github.com/facebookresearch/CommAI-env)
		- [A Roadmap towards Machine Intelligence](https://arxiv.org/abs/1511.08130)
- [General Video Game AI Competition](http://gvgai.net/index.php)
- Faster Reinforcement Learning Algorithms [Using human experience replays]:
	- [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952) - framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently
	- [Deep Exploration via Bootstrapped DQN](https://arxiv.org/abs/1602.04621)
	- [Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay](https://arxiv.org/abs/1607.05077)
- GoodAI:
	- [GoodAI Bibliography](https://www.goodai.com/research-inspirations)
	- [Framework for searching AGI](https://arxiv.org/pdf/1611.00685.pdf)
	- [GOOD version 1 AI AGENT DEVELOPMENT ROADMAP](https://media.wix.com/ugd/2f0a43_091d76d2b0354b0db4d88c3a57fdf76d.pdf)
- [Curriculum Learning](http://www.machinelearning.org/archive/icml2009/papers/119.pdf)
	- [Baby AI School](http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/BabyAISchool)
    - [Deep Architectures for Baby AI](http://www.cs.toronto.edu/~amnih/cifar/talks/bengio_tutorial.pdf) - Yoshua Bengio

#### References
- [Facebook AI director Yann LeCun: A Path to AI Keynote Speech - YT](https://www.youtube.com/watch?v=r7itbCYaSNQ)

#### Unprocessed Links:
- https://research.fb.com/category/facebook-ai-research-fair/
- https://www.general-ai-challenge.org/updates - Looks important
- https://www.quora.com/How-do-I-get-into-artificial-general-intelligence
- Godel Machines: https://arxiv.org/pdf/cs/0309048.pdf
- A Monte-Carlo AIXI Approximation: http://www.aaai.org/Papers/JAIR/Vol40/JAIR-4004.pdf
- https://sites.google.com/site/narswang/home/agi-introduction
- http://www.agi-society.org/resources/
- https://www.ibm.com/watson/developercloud/  - Watson cloud

#### Random Tools:
- RNN - Serial processing
- Memory - To store and retrive information
- Neural Turing machine - ability to programatically access and write and learn what to store and how to process
- Progressive learning i.e. using what I learnt earlier to learn learn new things
- Teacher - Student relation to learn things better
- A evaluation metric to check whether the things has been learnt or not
- Embeddings or vector representation are the way thought are stored 
- Trigger from vectors can spark a series of outputs that is imagination
- A learning system where there is a input, there is a output
- The system feels through the input like touch, 
- The system tries to learn what it sees by mimicing the same through its output
- The evaluation system makes sure it gets a proper representation or embedding i.e. can it consistently produce the same or simillar output for a particular input
- I see a apple and hear a apple then I try recreating the apple (in my mind) aided by trying to mimic the apple by drawing or pronouncing it by mouth
- So next time I want to learn to see about apple I can either imagine it or draw it 
or when I want to learn to hear about apple I can imagine the sound of apple or pronounce it
- Then the evaluation system can say whether my imagination is right or wrong
- Different embedding mechanisms - RNN char embedding, encoder-decode for machine translation, Auto Encoders, GAN
- Why was GAN found?
- The major concepts in mind
	- Memory
		- Heirarchcal memory systems. Direct storage of data, a representation of data and information stored as linked information like a graph i.e. bility to see simillar data together and distance data further away
		- Direct storate is fast to access and give you direct answers like a hash map
		- Work like a cache store frequently accessed or nearby (spatial) correlated of accessed data
		- Slower memory go through steps to get you to the actual output. Once accessed they probably go into cache
		- Probably slower memory are embedded representation of data
	- An embedded representation of data i.e. data is continously tried to be compressed by making them embedded representation continously
		- Ability to imagine by triggering and reconstructing it by 
	- Using a teacher - student kind of approach to learn
	- Ability to extend what is learnt earlier to newer things or generalise what it learnt earlier based on what it is learning new. So how does it do it
		- reimagines the earlier task to reproduce the outputs and tries to optimise the same or a extended network 
		- Probably use the imagine i.e. using embedding to trigger expected outputs for the previously learnt tasks and generate the required supervised learning data set to optimise along with the current learning task
		- Transfer learning approaches: 
			- [PathNet](https://medium.com/@thoszymkowiak/deepmind-just-published-a-mind-blowing-paper-pathnet-f72b1ed38d46)
			- [Evolution Statergies](https://blog.openai.com/evolution-strategies/)
		- [Elastic Weight Consolidation](http://www.wired.co.uk/article/deepmind-atari-learning-sequential-memory-ewc)
#### Key areas:
	- Memory
	- Attention
	- Concepts
	- Planning
	- Navigation
	- Imagination
#### Other research labs:
- Facebook AI Research:
	- [Blog](https://research.fb.com/)
	- [caffe2](https://caffe2.ai/blog/2017/04/18/caffe2-open-source-announcement.html)
	- [Building an effective dialog system](https://research.fb.com/the-long-game-towards-understanding-dialog/) - Looks important for involves details about AommAI Env
	- [FastText](https://github.com/facebookresearch/fastText) - Word Embeddings and Text classification - Probable use for RNN
- Deepmind:
	- [Github](https://github.com/deepmind)
	- [Blog](https://deepmind.com/)
	- [Deepmind Lab](https://deepmind.com/blog/open-sourcing-deepmind-lab/)
- OpenAi:
	- [](https://blog.openai.com/evolution-strategies/)
- [Journal of Artificial General Intelligence](https://www.degruyter.com/view/j/jagi)

- Minibank for saving money for kids
- ability to get good deals by buying things with friend near by and thus reducing cost of delivery. Linking neughbors together.
- Ability to generate movies faster with AI
- https://creativemarket.com/

Solution which seems to address a similar problem seems to be Neural Turing Machine/ Differentiable Neural Computers. I am yet to understand this. 

The rough direction for a possible solution, I had in mind is:
(1) A supervised learning neural network - CNN, LSTM or RNN based on what it is learning
    - CNN would be good for getting information about the whole picture in a multi-resolution approach. it could be image identification or paragraph sentiment analysis
    - While a serial or list based information interpretation would involve an LSTM or RNN. Like text analysis/generation, pixelRNN etc

(2) Then this network would get generalised more and more for different kind of tasks. Probably a generic CNN+RNN combination whose complexity could be increased by adding more layers to generalise more information. Like say there is a network which could understand numbers, by probably adding more layers (or increasing the num neurones in a particular layer), we increase make is understand alphabets, shapes etc.

(3) Ideally, an information storage and retrieval system could be approximated by directly storing the information in memory as a hash/key-value map i.e. we can either store the image directly with the corresponding inferences of the image. Thus if one wants to retrieve the information about the image they use this mapping to get the inferences or vice-versa. Probably like a graph.
           - Another way would be to store information as like in Generative Adversarial Models. Store the information about various types of images like cloud images as a trained network which can generate one based on input. (I have not fully understood GAN yet but the little I read seems to point in this direction) This could be something similar to imagination where we re-run a scenario to learn it or understand it better

(4) So we have a memory hierarchy which can have direct access [but limited in size like the key-value pair](short term memory) and a long-term representation like that of a GAN kind of network which can generate images based on the generalisation which has been stored on the earlier representations. Images in the short term would move to long term after they have been successfully trained and then would/could be deleted from the short term.

(5) Now we have a memory system which stores the information and a generalised supervised learning NN which can scale up or down.

(6) So a learning algorithm which tries to optimise the available resource like:
    - (a) time - how long does it take to retrieve an information or answer a query
    - (b) size of the network - how deep or wide should the network be to accommodate the available information (probably by information like underfitting and overfitting etc)
- This could be a probable application for RL. The reason I got pulled to this was the result published by google where Intelligent agents can play atari games by themselves followed by the "alphaGo" success. 


(7) So let's say we are training a machine to understand the following in increasing order of complexity (a) shapes (b) numbers  (c) addition etc....
- The algorithm could be:
    (a) training set for shapes with the corresponding test (could be like test questions behind a textbook) used to train a basic supervised NN
    (b) the data used for shapes could be stored directly as key value pairs
    (c) Then once the supervised NN is trained the data could then be stored as a representation in a GAN to reduce the space
    (d) Then the same supervised learning network could be used to train numbers.
    (e) We could try reusing the weights by transfer learning. 
    (f)  Then an RL algorithm could be run to optimise the number of NN units by running the GAN for generating the data required for retraining.
    (g) This process could be repeated for every new thing to train by gradually growing the supervised NN size and increasing the GANs